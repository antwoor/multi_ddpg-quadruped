<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# Да, подбери примеры коэффициентов основываясь на опыте и результатах экспериментов по PPO для локомоции из научных статей

Ниже пример «рабочего» набора коэффициентов для такой функции вознаграждения PPO‑локомоции квадрупеда, собранный по мотивам типичных настроек в работах по ходьбе на скоростях порядка 0.5–1 м/с и симуляторах MuJoCo/PyBullet (ант/ANYmal/Unitree‑подобные).[^3][^5][^7]

### Базовые условия

- Шаг симуляции: 0.01–0.02 c.
- Целевая скорость вперёд $v_x^{\text{cmd}} = 0.5\text{–}0.8$ м/с.
- Ожидаемый масштаб награды: суммарное $r_t$ в районе 0.2–1.0 на шаг при успешной ходьбе, заметно отрицательное при падении.[^3]


### Рекомендуемые коэффициенты

Используем форму из прошлого сообщения:

$$
\begin{aligned}
r_t =&
-k_v^{\text{lin}}\left[
(v_x - v_x^{\text{cmd}})^2
+ (v_y - v_y^{\text{cmd}})^2
\right]
-k_v^{\text{ang}}(\omega_z - \omega_z^{\text{cmd}})^2 \\
&-k_{\text{post}}(\phi^2 + \theta^2)
-k_h(h - h^{\text{ref}})^2
-k_\tau \sum_i \tau_i^2
-k_{\Delta a} \sum_i (a_{t,i} - a_{t-1,i})^2 \\
&+k_{\text{contact}}\left(
\frac{1}{N_\text{feet}}\sum_i c_i
\right)
+k_{\eta}\,\eta
+k_{\text{survive}}
-k_{\text{fall}}\cdot \mathbf{1}_{\text{fall}}.
\end{aligned}
$$

Пример значений:

- $k_v^{\text{lin}} = 2.0$
(при ошибке по скорости 0.5 м/с даёт штраф порядка $-0.5$, что сопоставимо с «ценой» заметной потери скорости).[^3]
- $k_v^{\text{ang}} = 0.5$
(обычно важность слежения по рысканию меньше, чем по поступательной скорости).
- $k_{\text{post}} = 5.0$
(штраф за наклон корпуса, чтобы $|\phi|,|\theta| \gtrsim 0.2$ рад уже сильно «били» по награде).
- $k_h = 10.0$
(выдерживает высоту в узком диапазоне вокруг $h^{\text{ref}}$, типичное отклонение 2–3 см даёт умеренный штраф).
- $k_\tau = 0.001\text{–}0.005$
(для моментов в диапазоне десятков Н·м это даёт штраф за «перегазовку», но не доминирует над скоростным термином; часто начинают с 0.001 и увеличивают после появления ходьбы).[^4]
- $k_{\Delta a} = 0.01$
(штраф за дёрганость действий; достаточно малый, чтобы не «убить» обучение, но сгладить движение).
- $k_{\text{contact}} = 0.1$
(простое поощрение за то, что хотя бы часть ног в контакте; при четырёх ногах даёт до +0.1 за шаг).
- $k_{\eta} = 1.0$
(поощрение за запас устойчивости; масштаб $\eta$ обычно порядка сантиметров, так что вклад умеренный).[^1]
- $k_{\text{survive}} = 0.05$
(постоянная награда за каждый шаг эпизода, стимулирует не падать).
- $k_{\text{fall}} = 5.0\text{–}10.0$
(разовый крупный штраф за падение; с горизонтом сотни шагов такого штрафа хватает, чтобы политика явно избегала падений).


### Как с этим работать на практике

- В начале обучения имеет смысл:
    - держать $k_\tau$ и $k_{\Delta a}$ ближе к нижней границе (0.001 и 0.005 соответственно),
    - сфокусироваться на $k_v^{\text{lin}}, k_{\text{post}}, k_h$, чтобы сначала вообще получить ходьбу.[^4]
- После того как робот стабильно идёт, можно постепенно увеличивать $k_\tau$ и $k_{\Delta a}$, чтобы сделать шаги более экономичными и плавными.
- Масштабируй все термины так, чтобы:
    - у «правильного» шага суммарное $r_t$ было положительным,
    - штрафы за типичные ошибки (0.2–0.3 м/с промаха по скорости, наклон корпуса 0.2–0.3 рад) были сопоставимы по модулю с величиной положительных частей.

Если пришлёшь конкретные диапазоны скоростей, шаг симуляции и типичные диапазоны моментов по суставам (из твоих логов Go1), могу подогнать коэффициенты точнее и переписать их компактной таблицей для раздела «Reward design».
<span style="display:none">[^10][^11][^2][^6][^8][^9]</span>

<div align="center">⁂</div>

[^1]: PZ_VKR_last-4.pdf

[^2]: https://arxiv.org/html/2512.06486v1

[^3]: https://www.scirp.org/journal/paperinformation?paperid=123401

[^4]: https://www.reddit.com/r/reinforcementlearning/comments/1m76ps1/quadruped_locomotion_with_ppo_how_to_move_forward/

[^5]: https://github.com/ansh1113/Quadruped-Locomotion-via-PPO

[^6]: https://ieeexplore.ieee.org/document/10450573/

[^7]: https://www.roboticsproceedings.org/rss21/p123.pdf

[^8]: https://www.instagram.com/p/DTUdpc9iSuf/

[^9]: https://www.reddit.com/r/reinforcementlearning/comments/1fbfk2i/ppo_quadruped_walking_in_pybulletbased_gym/

[^10]: https://www.reddit.com/r/reinforcementlearning/comments/1ik7dhn/training_quadrupeds_with_reinforcement_learning/

[^11]: https://www.reddit.com/r/reinforcementlearning/comments/1eqhwra/quadruped_rl_question/

